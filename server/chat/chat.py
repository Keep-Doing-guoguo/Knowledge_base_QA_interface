#!/usr/bin/env python
# coding=utf-8

"""
@author: zgw
@date: 2025/5/13 14:36
@source from: 
"""
from fastapi import Body
from sse_starlette.sse import EventSourceResponse
from configs import LLM_MODELS,TEMPERATURE
from langchain.chains import LLMChain
from langchain.callbacks import AsyncIteratorCallbackHandler
from typing import AsyncIterator
import asyncio
import json
from langchain.prompts.chat import ChatPromptTemplate
from typing import List,Optional,Union
from server.chat.utils import History
from langchain.prompts import PromptTemplate
from server.utils import get_prompt_template, get_ChatOpenAI,wrap_done
from server.memory.conversation_db_buffer_memory import ConversationBufferDBMemory
from server.db.repository import add_message_to_db
from server.callback_handler.conversation_callback_handler import ConversationCallbackHandler



async def chat(query:str = Body(...,description='',examples=['ä½ å¥½']),
               conversation_id:str = Body(...,description=''),
               history_len:int = Body(),
               history:Union[int,List[History]] = Body(),
               stream:bool = Body(),
               model_name:str = Body(),
               temperature:float = Body(),
               max_tokens :Optional[int] = Body(),
               prompt_name:str = Body()
               ):


    async def chat_iterator() -> AsyncIterator[str]:
        nonlocal history,max_tokens
        callback = AsyncIteratorCallbackHandler()
        callbacks = [callback]
        memory = None

        message_id = add_message_to_db(chat_type='llm_chat',conversation_id=conversation_id)

        conversation_callback = ConversationCallbackHandler(conversation_id=conversation_id, message_id=message_id,
                                                            chat_type="llm_chat",
                                                            query=query)  # è¿™é‡Œæ˜¯å½“èŠå¤©ç»“æŸåå°†å›è°ƒå‡½æ•°ï¼Œç„¶åä¿å­˜å†å²èŠå¤©è®°å½•ã€‚
        callbacks.append(conversation_callback)
        if isinstance(max_tokens, int) and max_tokens <= 0:
            max_tokens = None
        print('debug3')
        model = get_ChatOpenAI(
            model_name=model_name,
            temperature=temperature,
            max_tokens=max_tokens,
            callbacks=callbacks,
        )

        if history:#è¡¨ç¤ºä»å‰ç«¯æ‹¿å‡ºæ¥çš„èŠå¤©è®°å½•
            history = [History.from_data(h) for h in history]#å…ˆæ‹¿åˆ°æ•°æ®è¿›è¡Œè½¬å˜
            prompt_template = get_prompt_template('llm_chat',prompt_name)
            input_msg = History(role='user',content=prompt_template).to_msg_template(False)
            chat_prompt = ChatPromptTemplate.from_messages(
                [i.to_msg_template() for i in history] + [input_msg]
            )
        elif conversation_id and history_len > 0:#è¡¨ç¤ºä»æ•°æ®åº“ä¸­æ‹¿çš„èŠå¤©è®°å½•

            prompt = get_prompt_template('llm_chat','with_history')
            chat_prompt = PromptTemplate.from_template(prompt)
            memory = ConversationBufferDBMemory(conversation_id=conversation_id,llm=model,message_limit = history_len)

        else:#è¡¨ç¤ºæ²¡æœ‰èŠå¤©è®°å½•
            prompt_template = get_prompt_template('llm_chat',prompt_name)
            input_msg = History(role='user',content=prompt_template).to_msg_template(False)
            chat_prompt = ChatPromptTemplate.from_messages([input_msg])

        chain = LLMChain(prompt=chat_prompt,llm=model,memory=memory)#LLMChain å†…éƒ¨ä¼šè‡ªåŠ¨è°ƒç”¨ .format() æˆ– .format_messages()ï¼
        chain = asyncio.create_task(
            wrap_done(
                chain.acall({"input":query}),
                callback.done,)
        )#è¿™é‡Œæ˜¯åˆ›å»ºä¿©ä¸ªä»»åŠ¡ã€‚

        if stream:
            async for token in callback.aiter():
                yield json.dumps({"text":token,"message_id":message_id},ensure_ascii=False)

        else:

            answer = ""
            async for token in callback.aiter():
                answer+=token
            yield json.dumps({"text":token,"message_id":message_id},ensure_ascii=False)
        await chain


    return EventSourceResponse(chat_iterator())
'''
ğŸ asyncio.run()
ç›¸å½“äºâ€œå¼€èµ›â€ â†’ å¯åŠ¨æ•´ä¸ªå¼‚æ­¥æ¯”èµ›ï¼ˆäº‹ä»¶å¾ªç¯ï¼‰
ğŸƒ asyncio.create_task()
ç›¸å½“äºâ€œè®©æŸä¸ªé€‰æ‰‹å¼€å§‹è·‘â€ â†’ åœ¨æ¯”èµ›è¿‡ç¨‹ä¸­å¹¶å‘å¤šä¸ªé€‰æ‰‹



import asyncio

async def main():
    print("Hello asyncio.run")

asyncio.run(main())
ï¼
import asyncio

async def main():
    print("Hello run_until_complete")

loop = asyncio.new_event_loop()
asyncio.set_event_loop(loop)
loop.run_until_complete(main())
loop.close()



'''