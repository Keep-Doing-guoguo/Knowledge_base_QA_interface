#!/usr/bin/env python
# coding=utf-8

"""
@author: zgw
@date: 2025/10/10 14:28
@source from: 
"""
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ§åˆ¶å°å¯è°ƒè¯•ç‰ˆçŸ¥è¯†åº“é—®ç­”
åŒæ­¥æ‰§è¡Œï¼Œæ—  FastAPI ä¾èµ–ã€‚
"""

import json
from urllib.parse import urlencode
from typing import List, Optional

from langchain.chains import LLMChain
from langchain.prompts.chat import ChatPromptTemplate
from langchain.callbacks.base import BaseCallbackHandler

from server.utils import get_ChatOpenAI, get_prompt_template
from server.chat.utils import History
from server.knowledge_base.kb_service.base import KBServiceFactory
from server.knowledge_base.kb_doc_api import search_docs
from server.reranker.reranker import LangchainReranker
from configs import (
    VECTOR_SEARCH_TOP_K,
    SCORE_THRESHOLD,
    TEMPERATURE,
    USE_RERANKER,
    RERANKER_MODEL,
    RERANKER_MAX_LENGTH,
    MODEL_PATH,
)


def knowledge_base_chat_console(
    query: str,
    knowledge_base_name: str = "samples",
    top_k: int = VECTOR_SEARCH_TOP_K,
    score_threshold: float = SCORE_THRESHOLD,
    history: Optional[List[History]] = None,
    model_name: str = "qwen-plus",
    temperature: float = TEMPERATURE,
    max_tokens: Optional[int] = None,
    prompt_name: str = "default",
):
    """
    åŒæ­¥æ‰§è¡Œç‰ˆæœ¬ï¼šæ–¹ä¾¿åœ¨ PyCharm / VSCode æ§åˆ¶å°è°ƒè¯•ã€‚
    """
    print(f"\nğŸ” å¼€å§‹çŸ¥è¯†åº“é—®ç­”ï¼š{query}")

    kb = KBServiceFactory.get_service_by_name(knowledge_base_name)
    if kb is None:
        print(f"âŒ æœªæ‰¾åˆ°çŸ¥è¯†åº“ï¼š{knowledge_base_name}")
        return

    if history is None:
        history = []

    # åˆå§‹åŒ–æ¨¡å‹
    model = get_ChatOpenAI(
        model_name=model_name,
        temperature=temperature,
        max_tokens=max_tokens,
        callbacks=[],  # å¯è‡ªå®šä¹‰æ‰“å°å›è°ƒ
    )

    # å‘é‡æ£€ç´¢
    print("ğŸ“š æ£€ç´¢çŸ¥è¯†åº“ä¸­ç›¸å…³æ–‡æ¡£...")
    docs = search_docs(
        query=query,
        knowledge_base_name=knowledge_base_name,
        top_k=top_k,
        score_threshold=score_threshold,
    )
    print(f"æ£€ç´¢ç»“æœæ•°é‡: {len(docs)}")

    # Reranker æ’åº
    if USE_RERANKER:
        reranker_model_path = MODEL_PATH["reranker"].get(RERANKER_MODEL, "BAAI/bge-reranker-large")
        reranker_model = LangchainReranker(
            top_n=top_k,
            device=None,
            max_length=RERANKER_MAX_LENGTH,
            model_name_or_path=reranker_model_path,
        )
        docs = reranker_model.compress_documents(documents=docs, query=query)
        print(f"ğŸ” Reranker æ’åºåæ–‡æ¡£æ•°é‡: {len(docs)}")

    # ç»„è£…ä¸Šä¸‹æ–‡
    context = "\n".join([doc.page_content for doc in docs])
    if len(docs) == 0:
        prompt_template = get_prompt_template("knowledge_base_chat", "empty")
    else:
        prompt_template = get_prompt_template("knowledge_base_chat", prompt_name)

    input_msg = History(role="user", content=prompt_template).to_msg_template(False)#è½¬å˜ä¸ºChatMessagePromptTemplateæ¨¡ç‰ˆ
    chat_prompt = ChatPromptTemplate.from_messages([i.to_msg_template() for i in history] + [input_msg])#è¿™é‡Œå†å°†å…¶è½¬å˜ä¸ºChatPrompttemplateæ¨¡ç‰ˆ
    chain = LLMChain(prompt=chat_prompt, llm=model)

    # æ‰§è¡Œè°ƒç”¨
    print("\nğŸ¤– æ­£åœ¨è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”...")
    result = chain.run({"context": context, "question": query})
    print("\nâœ… æ¨¡å‹å›ç­”ï¼š")
    print(result)

    # è¾“å‡ºå‚è€ƒæ–‡æ¡£
    print("\nğŸ“– å‚è€ƒæ–‡æ¡£ï¼š")
    for i, doc in enumerate(docs):
        print(f"[{i + 1}] æ¥æº: {doc.metadata.get('source')}")
        print(f"å†…å®¹æ‘˜è¦: {doc.page_content[:200]}...\n")

    return result


if __name__ == "__main__":
    query = 'æ–°ä¹¡å·¥ç¨‹å­¦é™¢'
    knowledge_base_chat_console(query)